{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.5, random_state=0)\n",
    "\n",
    "# model \n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "\n",
    "tuned_parameters = {\n",
    "                    'n_estimators': [50, 100, 200]\n",
    "#                     ,'criterion': ['gini', 'entropy']\n",
    "#                     ,'max_depth': [2, 5]\n",
    "#                     ,'max_features': ['log2', 'sqrt', 'int']\n",
    "#                     ,'bootstrap': [True, False]\n",
    "#                     ,'warm_start': [True, False]\n",
    "                    }\n",
    "\n",
    "scores = ['precision']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(clf, tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
    "                                                    iris.target,\n",
    "                                                    random_state=0)\n",
    "print(\"Size of training set:{} size of testing set:{}\".format(\n",
    "    X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "####   grid search start\n",
    "best_score = 0\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        svm = SVC(gamma=gamma, C=C)  #对于每种参数可能的组合，进行一次训练；\n",
    "        svm.fit(X_train, y_train)\n",
    "        score = svm.score(X_test, y_test)\n",
    "        if score > best_score:  #找到表现最好的参数\n",
    "            best_score = score\n",
    "            best_parameters = {'gamma': gamma, 'C': C}\n",
    "####   grid search end\n",
    "print(\"Best score:{:.2f}\".format(best_score))\n",
    "print(\"Best parameters:{}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval,X_test,y_trainval,y_test = train_test_split(iris.data,iris.target,random_state=0)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_trainval,y_trainval,random_state=1)\n",
    "print(\"Size of training set:{} size of validation set:{} size of testing set:{}\".format(X_train.shape[0],X_val.shape[0],X_test.shape[0]))\n",
    "\n",
    "best_score = 0.0\n",
    "for gamma in [0.001,0.01,0.1,1,10,100]:\n",
    "    for C in [0.001,0.01,0.1,1,10,100]:\n",
    "        svm = SVC(gamma=gamma,C=C)\n",
    "        svm.fit(X_train,y_train)\n",
    "        score = svm.score(X_val,y_val)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'gamma':gamma,'C':C}\n",
    "svm = SVC(**best_parameters) #使用最佳参数，构建新的模型\n",
    "svm.fit(X_trainval,y_trainval) #使用训练集和验证集进行训练，more data always results in good performance.\n",
    "test_score = svm.score(X_test,y_test) # evaluation模型评估\n",
    "print(\"Best score on validation set:{:.2f}\".format(best_score))\n",
    "print(\"Best parameters:{}\".format(best_parameters))\n",
    "print(\"Best score on test set:{:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "best_score = 0.0\n",
    "for gamma in [0.001,0.01,0.1,1,10,100]:\n",
    "    for C in [0.001,0.01,0.1,1,10,100]:\n",
    "        svm = SVC(gamma=gamma,C=C)\n",
    "        scores = cross_val_score(svm,X_trainval,y_trainval,cv=5) #5折交叉验证\n",
    "        score = scores.mean() #取平均数\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {\"gamma\":gamma,\"C\":C}\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval,y_trainval)\n",
    "test_score = svm.score(X_test,y_test)\n",
    "print(\"Best score on validation set:{:.2f}\".format(best_score))\n",
    "print(\"Best parameters:{}\".format(best_parameters))\n",
    "print(\"Score on testing set:{:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证经常与网格搜索进行结合，作为参数评价的一种方法，这种方法叫做grid search with cross validation。sklearn因此设计了一个这样的类GridSearchCV，这个类实现了fit，predict，score等方法，被当做了一个estimator，使用fit方法，该过程中：（1）搜索到最佳参数；（2）实例化了一个最佳参数的estimator；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#把要调整的参数以及其候选值 列出来；\n",
    "param_grid = {\"gamma\":[0.001,0.01,0.1,1,10,100],\n",
    "             \"C\":[0.001,0.01,0.1,1,10,100]}\n",
    "print(\"Parameters:{}\".format(param_grid))\n",
    "\n",
    "grid_search = GridSearchCV(SVC(),param_grid,cv=5) #实例化一个GridSearchCV类\n",
    "X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=10)\n",
    "grid_search.fit(X_train,y_train) #训练，找到最优的参数，同时使用最优的参数实例化一个新的SVC estimator。\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_test,y_test)))\n",
    "print(\"Best parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best score on train set:{:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lgb调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "parameters = {'learning_rate': [0.01, 0.1, 1], 'n_estimators': [20, 40]}\n",
    "clf = GridSearchCV(clf, parameters, cv=5)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "print('Best parameters found by grid search are:', clf.best_params_)\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "print(\"LGBMRegressor GridSearchCV test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用lr_reg和lgb_reg进行融合预测\n",
    "clf_list = [lr_reg, lgb_reg]\n",
    "\n",
    "##很容易过拟合\n",
    "pred = stacking_pred(x_train, y_train, x_valid, kf, clf_list, label_split=None, clf_fin=\"lgb\", if_concat_origin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 穷举网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split  # 切分数据\n",
    "# 切分数据 训练数据80% 验证数据20%\n",
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    train, target, test_size=0.2, random_state=0)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [20, 50, 100], 'max_depth': [1, 2, 3]}\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv=3, verbose=2)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = roc_auc_score(test_target, clf.predict(test_data))\n",
    "\n",
    "print(\"RandomForestClassifier GridSearchCV test AUC:   \", score_test)\n",
    "print(\"最优参数:\")\n",
    "print(clf.best_params_)\n",
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split  # 切分数据\n",
    "# 切分数据 训练数据80% 验证数据20%\n",
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    train, target, test_size=0.2, random_state=0)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [10, 20, 30, 50], 'max_depth': [1, 2, 3]}\n",
    "\n",
    "clf = RandomizedSearchCV(model, parameters, cv=3, verbose=2)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = roc_auc_score(test_target, clf.predict(test_data))\n",
    "\n",
    "print(\"RandomForestClassifier RandomizedSearchCV test AUC:   \", score_test)\n",
    "print(\"最优参数:\")\n",
    "print(clf.best_params_)\n",
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM 网格调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM 调参次序：  \n",
    "第一步：学习率和迭代次数  \n",
    "第二步：确定max_depth和num_leaves  \n",
    "第三步：确定min_data_in_leaf和max_bin in  \n",
    "第四步：确定feature_fraction、bagging_fraction、bagging_freq  \n",
    "第五步：确定lambda_l1和lambda_l2  \n",
    "第六步：确定 min_split_gain   \n",
    "第七步：降低学习率，增加迭代次数，验证模型  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一步：学习率和迭代次数\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 切分数据 训练数据80% 验证数据20%\n",
    "# 为了加快速度CV选的3，其实一般用5\n",
    "# 因为每训练一次耗时很多，所以每个参数的选项不多，间隔比较大，正式的时候应该是比较多，间隔比较细的\n",
    "# 本次只是演示，所以如果最好参数位于区间的边缘也就直接用了，其实如果最好参数在边缘，需要重新再搜索。\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       learning_rate=0.1,\n",
    "                       max_depth=5,\n",
    "                       bagging_fraction=0.8,\n",
    "                       feature_fraction=0.8)\n",
    "parameters = {'n_estimators': [100, 150, 175, 200, 225, 250]}\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv=3, verbose=2)\n",
    "clf.fit(train_data, train_target)\n",
    "score_test = roc_auc_score(test_target, clf.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM GridSearchCV AUC Score:   \", score_test)\n",
    "print(\"最优参数:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二步：确定max_depth和num_leaves\n",
    "#n_estimators=200\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       learning_rate=0.1,\n",
    "                       bagging_fraction=0.8,\n",
    "                       feature_fraction=0.8)\n",
    "parameters = {'max_depth': range(4, 8, 1), 'num_leaves': range(10, 150, 10)}\n",
    "#parameters={'max_depth': range(4,8,2), 'num_leaves':range(10, 100, 20)}\n",
    "clf = GridSearchCV(model, parameters, cv=3, verbose=2)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = roc_auc_score(test_target, clf.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM GridSearchCV AUC Score:   \", score_test)\n",
    "print(\"最优参数:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据结果取 max_depth=6, num_leaves=40 也都这么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第三步：确定min_data_in_leaf和max_bin in\n",
    "#已经确认内容：\n",
    "#n_estimators=200\n",
    "#{'max_depth': 6, 'num_leaves': 40}\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=6,\n",
    "                       num_leaves=40,\n",
    "                       learning_rate=0.1,\n",
    "                       bagging_fraction=0.8,\n",
    "                       feature_fraction=0.8)\n",
    "\n",
    "#parameters={'max_bin': range(100,500,50),'min_data_in_leaf':range(100,150,10)}\n",
    "parameters = {\n",
    "    'max_bin': range(100, 500, 100),\n",
    "    'min_data_in_leaf': range(100, 150, 50)\n",
    "}\n",
    "#调高 verbose可以看到更多信息\n",
    "clf = GridSearchCV(model, parameters, cv=3, verbose=3)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = roc_auc_score(test_target, clf.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM GridSearchCV AUC Score:   \", score_test)\n",
    "print(\"最优参数:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第四步：确定feature_fraction、bagging_fraction、bagging_freq\n",
    "#已经确认内容：\n",
    "#n_estimators=200\n",
    "#{'max_depth': 6, 'num_leaves': 40}\n",
    "#{'max_bin': 400, 'min_data_in_leaf': 120}\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=6,\n",
    "                       num_leaves=40,\n",
    "                       max_bin=400,\n",
    "                       min_data_in_leaf=120,\n",
    "                       learning_rate=0.1,\n",
    "                       bagging_fraction=0.8,\n",
    "                       feature_fraction=0.8)\n",
    "\n",
    "parameters = {\n",
    "    'feature_fraction': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'bagging_freq': range(0, 10, 2)\n",
    "}\n",
    "\n",
    "#调高 verbose可以看到更多信息\n",
    "clf = GridSearchCV(model, parameters, cv=3, verbose=3)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = roc_auc_score(test_target, clf.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM GridSearchCV AUC Score:   \", score_test)\n",
    "print(\"最优参数:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第五步：确定lambda_l1和lambda_l2\n",
    "#已经确认内容：\n",
    "#n_estimators=200\n",
    "#{'max_depth': 6, 'num_leaves': 40}\n",
    "#{'max_bin': 400, 'min_data_in_leaf': 120}\n",
    "#{'bagging_fraction': 0.9, 'bagging_freq': 4, 'feature_fraction': 0.6}\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=6,\n",
    "                       num_leaves=40,\n",
    "                       max_bin=400,\n",
    "                       min_data_in_leaf=120,\n",
    "                       learning_rate=0.1,\n",
    "                       bagging_freq=4,\n",
    "                       bagging_fraction=0.9,\n",
    "                       feature_fraction=0.6)\n",
    "\n",
    "parameters = {\n",
    "    'lambda_l1': [1e-5, 1e-3, 1e-1, 0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "    'lambda_l2': [1e-5, 1e-3, 1e-1, 0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "#调高 verbose可以看到更多信息\n",
    "clf = GridSearchCV(model, parameters, cv=3, verbose=3)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = roc_auc_score(test_target, clf.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM GridSearchCV AUC Score:   \", score_test)\n",
    "print(\"最优参数:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第六步：确定 min_split_gain\n",
    "#已经确认内容：\n",
    "#n_estimators=200\n",
    "#{'max_depth': 6, 'num_leaves': 40}\n",
    "#{'max_bin': 400, 'min_data_in_leaf': 120}\n",
    "#{'bagging_fraction': 0.9, 'bagging_freq': 4, 'feature_fraction': 0.6}\n",
    "#{'lambda_l1': 1e-05, 'lambda_l2': 1e-05}\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=6,\n",
    "                       num_leaves=40,\n",
    "                       max_bin=400,\n",
    "                       min_data_in_leaf=120,\n",
    "                       learning_rate=0.1,\n",
    "                       lambda_l1=1e-05,\n",
    "                       lambda_l2=1e-05,\n",
    "                       bagging_freq=4,\n",
    "                       bagging_fraction=0.9,\n",
    "                       feature_fraction=0.6)\n",
    "\n",
    "parameters = {\n",
    "    'min_split_gain': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "#调高 verbose可以看到更多信息\n",
    "clf = GridSearchCV(model, parameters, cv=3, verbose=3)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = roc_auc_score(test_target, clf.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM GridSearchCV AUC Score:   \", score_test)\n",
    "print(\"最优参数:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第七步：降低学习率，增加迭代次数，验证模型\n",
    "#已经确认内容：\n",
    "#n_estimators=200\n",
    "#{'max_depth': 6, 'num_leaves': 40}\n",
    "#{'max_bin': 400, 'min_data_in_leaf': 120}\n",
    "#{'bagging_fraction': 0.9, 'bagging_freq': 4, 'feature_fraction': 0.6}\n",
    "#{'lambda_l1': 1e-05, 'lambda_l2': 1e-05}\n",
    "#{'min_split_gain': 0.0}\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=6,\n",
    "                       num_leaves=40,\n",
    "                       max_bin=400,\n",
    "                       min_data_in_leaf=120,\n",
    "                       learning_rate=0.1,\n",
    "                       lambda_l1=1e-05,\n",
    "                       lambda_l2=1e-05,\n",
    "                       min_split_gain=0.0,\n",
    "                       bagging_freq=4,\n",
    "                       bagging_fraction=0.9,\n",
    "                       feature_fraction=0.6)\n",
    "\n",
    "model.fit(train_data, train_target)\n",
    "score_test = roc_auc_score(test_target, model.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM learning rate 0.1 AUC Score:   \", score_test)\n",
    "\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=6,\n",
    "                       num_leaves=40,\n",
    "                       max_bin=400,\n",
    "                       min_data_in_leaf=120,\n",
    "                       learning_rate=0.05,\n",
    "                       lambda_l1=1e-05,\n",
    "                       lambda_l2=1e-05,\n",
    "                       min_split_gain=0.0,\n",
    "                       bagging_freq=4,\n",
    "                       bagging_fraction=0.9,\n",
    "                       feature_fraction=0.6)\n",
    "\n",
    "model.fit(train_data, train_target)\n",
    "score_test = roc_auc_score(test_target, model.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM learning rate 0.05 AUC Score:   \", score_test)\n",
    "\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=6,\n",
    "                       num_leaves=40,\n",
    "                       max_bin=400,\n",
    "                       min_data_in_leaf=120,\n",
    "                       learning_rate=0.01,\n",
    "                       lambda_l1=1e-05,\n",
    "                       lambda_l2=1e-05,\n",
    "                       min_split_gain=0.0,\n",
    "                       bagging_freq=4,\n",
    "                       bagging_fraction=0.9,\n",
    "                       feature_fraction=0.6)\n",
    "\n",
    "model.fit(train_data, train_target)\n",
    "score_test = roc_auc_score(test_target, model.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM learning rate 0.01 AUC Score:   \", score_test)\n",
    "\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "                       objective='binary',\n",
    "                       metrics='auc',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=6,\n",
    "                       num_leaves=40,\n",
    "                       max_bin=400,\n",
    "                       min_data_in_leaf=120,\n",
    "                       learning_rate=0.005,\n",
    "                       lambda_l1=1e-05,\n",
    "                       lambda_l2=1e-05,\n",
    "                       min_split_gain=0.0,\n",
    "                       bagging_freq=4,\n",
    "                       bagging_fraction=0.9,\n",
    "                       feature_fraction=0.6)\n",
    "\n",
    "model.fit(train_data, train_target)\n",
    "score_test = roc_auc_score(test_target, model.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"LightGBM learning rate 0.005 AUC Score:   \", score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier()\n",
    "\n",
    "model.fit(train_data, train_target)\n",
    "score_test = roc_auc_score(test_target, model.predict_proba(test_data)[:, 1])\n",
    "\n",
    "print(\"默认参数 AUC Score:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优参数 model = LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',n_estimators=200,max_depth=6,num_leaves=40,\n",
    "max_bin=400,min_data_in_leaf=120,\n",
    "learning_rate=0.05,\n",
    "lambda_l1=1e-05,lambda_l2=1e-05,min_split_gain=0.0,\n",
    "bagging_freq=4, bagging_fraction = 0.9,feature_fraction = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f3特征\n",
    "train = train_f3.copy()\n",
    "train.head()\n",
    "print('默认参数')\n",
    "classifier_df_score(train, 'LGB', 5)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'eval_metric': 'auc',\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 5,\n",
    "    'num_leaves': 40,\n",
    "    'max_bin': 400,\n",
    "    'min_data_in_leaf': 120,\n",
    "    'learning_rate': 0.1,\n",
    "    'lambda_l1': 1e-05,\n",
    "    'lambda_l2': 1e-05,\n",
    "    'min_split_gain': 0.0,\n",
    "    'bagging_freq': 4,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'feature_fraction': 0.6,\n",
    "    'seed': 1024,\n",
    "    'n_thread': 12\n",
    "}\n",
    "print('调参后')\n",
    "classifier_df_score(train, 'LGB', 5, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比发现调参后的结果比默认参数有所提高，不过不是高很多，比不上特征对结果的影响。而且因为调参只能再测试集上作，有的时候调参造成过拟，调参后线上成绩可能反而会下降。所以调参一般都是在比赛后期再做，前期主要是特征和模型的选择。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:all] *",
   "language": "python",
   "name": "conda-env-all-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
